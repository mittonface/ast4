{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import mdptoolbox, mdptoolbox.example\n",
    "import matplotlib.pyplot as plt\n",
    "from Tkinter import *\n",
    "import time\n",
    "\n",
    "    \n",
    "class RLMaze(object):\n",
    "    \n",
    "    def __init__(self, maze=None, start=None, goal=None, R=None):\n",
    "        if maze is None:\n",
    "            self.maze = np.asarray([\n",
    "                [0,1,0,1,0,1,1,1,0,1,1,0,1,0,0,0,1,1,0,1,1,1,0,1,1,0,0,0,1,1,1,0,1,1,0,0,0,0,0,0,1,1,0,1,1,0,0,1,1,0],\n",
    "                [0,1,0,1,0,1,1,0,1,1,1,0,1,1,1,1,0,1,1,1,0,0,0,0,1,1,1,0,0,0,1,1,1,0,0,0,0,0,1,1,0,0,1,0,1,0,0,0,1,1],\n",
    "                [0,1,1,0,1,1,0,0,1,1,1,0,0,1,0,0,0,1,1,1,0,1,1,0,0,0,0,0,0,1,1,1,0,0,0,1,0,1,1,0,0,1,0,0,0,0,0,0,0,0],\n",
    "                [0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0],\n",
    "                [0,0,2,2,2,2,0,0,0,0,2,0,1,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,1,0,0,0,0,1,1,1,1,1,1,1,1,0,0,0,1,0,0,0,1,0],\n",
    "                [0,1,0,1,0,1,1,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,1,1,0,1,0,1,0,1,1,1,1,1,0,1,1,0,0,0,0,0,0,0,0,1,1,0,1],\n",
    "                [1,0,1,0,1,0,1,0,1,1,1,1,0,0,0,0,1,1,0,1,1,0,0,0,0,1,1,0,1,1,0,1,1,0,0,0,1,1,0,1,1,0,0,0,1,1,1,0,0,0],\n",
    "                [0,1,0,1,0,1,1,0,1,1,0,1,0,1,0,1,0,0,0,1,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,1,0,0,0,0],\n",
    "                [0,1,0,0,0,0,1,0,1,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,1,2,2,2,2,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0],\n",
    "                [0,1,0,1,1,1,1,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,1,1,1,1,1,1,0,0,0,0,0,0,0,0,1,0,0,0,0],\n",
    "                [0,0,0,1,1,0,1,1,1,0,0,0,0,2,2,2,2,0,1,0,0,0,0,0,1,0,0,0,0,1,1,1,1,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0],\n",
    "                [0,1,1,1,1,1,0,0,0,0,0,0,0,2,2,2,2,0,0,0,1,0,0,2,2,2,1,0,0,0,1,1,1,1,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1],\n",
    "                [0,1,0,1,1,1,1,1,1,1,1,1,1,1,1,1,1,0,0,1,1,0,0,0,0,0,0,0,0,1,2,2,0,0,0,0,1,1,1,0,1,0,1,0,1,0,1,0,1,1],\n",
    "                [0,1,1,0,1,0,1,1,1,0,1,1,1,0,1,1,0,0,0,0,0,0,0,1,1,1,1,1,0,0,0,0,0,0,0,0,1,0,1,0,1,0,1,1,1,1,1,1,1,1],\n",
    "                [0,0,0,0,0,0,0,0,1,1,1,0,1,0,0,0,1,1,1,1,1,0,0,0,0,0,0,0,1,0,0,0,1,1,1,0,0,1,1,1,1,1,0,0,0,0,0,0,0,0],\n",
    "                [0,1,0,1,0,1,1,1,0,1,1,0,1,0,0,0,1,1,0,1,1,1,0,1,1,0,0,0,1,1,1,0,1,1,0,0,0,0,0,0,1,1,0,1,1,0,0,1,1,0],\n",
    "                [0,1,0,1,0,1,1,0,1,1,1,0,1,1,1,1,0,1,1,1,0,0,0,0,1,1,1,0,0,0,1,1,1,0,0,0,0,0,1,1,0,0,1,0,1,0,0,0,1,1],\n",
    "                [0,1,1,0,1,1,0,0,1,1,1,0,0,1,0,0,0,1,1,1,0,1,1,0,0,2,0,0,0,1,1,1,0,0,0,1,0,1,1,0,0,1,0,0,0,0,0,0,0,0],\n",
    "                [0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0],\n",
    "                [0,0,0,0,0,1,0,0,0,0,1,0,1,0,0,0,0,0,0,0,1,2,0,2,2,2,2,0,1,0,0,0,0,1,1,1,1,1,1,1,1,0,0,0,1,0,0,0,1,0],\n",
    "                [0,1,0,1,0,1,1,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,1,1,0,1,0,1,0,1,1,0,1,1,0,1,1,0,0,0,0,0,0,0,2,2,2,2,1],\n",
    "                [1,0,1,0,1,0,1,0,1,1,1,1,0,0,0,0,1,1,0,1,1,0,0,0,0,1,1,0,1,1,0,1,0,0,0,0,1,1,0,1,1,0,0,0,1,1,1,0,0,0],\n",
    "                [0,1,0,1,0,1,1,0,1,1,0,1,0,1,0,0,2,0,0,1,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,1,0,0,0,0],\n",
    "                [0,1,0,0,0,0,1,0,1,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,1,0,0,0,0,2,2,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0],\n",
    "                [0,1,0,1,1,1,1,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,1,1,1,1,1,1,0,0,0,0,0,0,0,0,1,0,0,0,0],\n",
    "                [0,0,0,1,1,0,1,1,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,1,1,1,1,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0],\n",
    "                [0,1,1,1,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,1,0,0,0,1,1,1,1,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1],\n",
    "                [0,1,0,1,1,1,1,1,1,1,1,1,1,1,1,1,1,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,1,0,1,0,1,0,1,0,1,0,1,1],\n",
    "                [0,1,1,0,1,0,1,1,1,0,1,1,1,0,1,1,0,0,0,0,0,0,0,1,1,1,1,1,0,0,0,0,0,0,0,0,1,0,1,0,1,0,1,1,1,1,1,1,1,1],\n",
    "                [0,0,0,0,0,0,0,0,1,1,1,0,1,0,0,0,1,1,1,1,1,0,0,0,0,0,0,0,1,0,0,0,1,1,1,0,0,1,1,1,1,1,0,0,0,0,0,0,0,0],\n",
    "                [0,0,0,0,0,0,1,0,1,1,1,1,1,1,1,0,1,1,1,1,1,1,1,1,0,2,0,0,0,0,0,0,0,0,1,1,0,0,0,1,1,0,0,0,1,1,1,1,1,1],\n",
    "                [1,0,0,0,0,1,1,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,1,1,0,1,1,0,0,0,0,0,0],\n",
    "                [1,0,0,0,0,1,1,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,1,1,0,1,1,0,0,0,0,0,0],\n",
    "                [1,0,0,0,0,1,1,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,1,1,0,1,1,0,0,0,0,0,0],\n",
    "                [1,0,0,0,0,1,1,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,2,0,1,0,0,0,1,1,0,1,1,0,0,0,0,0,0],\n",
    "                [1,0,0,0,0,1,1,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,2,2,2,2,2,2,0,0,2,0,1,0,0,0,1,1,0,1,1,0,0,0,0,0,0],\n",
    "                [1,0,0,0,0,1,1,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,2,0,1,0,0,0,1,1,0,1,1,0,0,0,0,0,0],\n",
    "                [1,0,0,0,0,1,1,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,2,0,1,0,0,0,1,1,0,1,1,0,0,0,0,0,0],\n",
    "                [0,1,1,0,0,0,0,0,0,0,0,1,1,0,0,1,0,0,1,0,0,0,0,1,0,0,1,1,1,1,1,1,0,0,0,0,0,2,2,2,1,0,0,0,0,1,0,0,0,1],\n",
    "                [0,1,1,0,0,0,0,0,0,1,0,1,0,0,0,0,0,1,1,1,0,0,0,1,0,1,1,1,1,1,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0],\n",
    "                [0,1,0,1,0,1,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,1,1,1,1,1,1,1,1,1,0,0,0,0,0,1,0,0,1,1,1,0,1,1,1],\n",
    "                [0,0,0,0,1,0,0,1,0,0,0,1,1,1,1,1,1,1,1,1,0,0,0,0,0,0,0,0,2,2,2,2,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0],\n",
    "                [0,1,0,1,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,1,1,1,1,1,1,1,1,1,1,1,0,1],\n",
    "                [0,0,0,0,0,0,0,1,1,0,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,1],\n",
    "                [0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,2,2,2,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1],\n",
    "                [1,0,0,1,0,1,1,1,0,1,1,0,1,1,1,1,1,1,0,1,1,1,0,1,0,0,1,0,1,0,1,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,0],\n",
    "                [0,1,1,1,1,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,1,1,1,1,1,1,1,0,1,1,0,1,1,0,1,1,1,1,0,1,1,1,0,0,0,0,0,0,1,0],\n",
    "                [0,0,0,1,0,1,0,1,1,0,1,1,0,0,0,2,2,2,2,0,0,0,0,0,1,0,1,0,1,0,0,1,0,1,0,1,0,1,0,0,0,0,0,0,1,1,1,0,0,0],\n",
    "                [0,1,1,0,1,1,1,1,1,0,1,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,1,1,1,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,1,1,0],\n",
    "                [0,1,1,0,1,1,0,0,0,0,1,1,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,1,1,2,3]\n",
    "                ])\n",
    "        else:\n",
    "            self.maze=maze\n",
    "\n",
    "\n",
    "        self.x_bound = self.maze.shape[0] - 1\n",
    "        self.y_bound = self.maze.shape[1] - 1\n",
    "        self.n_states = self.maze.shape[0] * self.maze.shape[1]\n",
    "        self.q_iters = 0\n",
    "\n",
    "        if start is None:\n",
    "            self.start = (0,0)\n",
    "        else:\n",
    "            self.start = start\n",
    "            \n",
    "        if goal is None:\n",
    "            self.goal = (49, 49)\n",
    "        else:\n",
    "            self.goal = goal\n",
    "            \n",
    "        if R is None:\n",
    "            self.reward_vals = [10, -0.05, -1]\n",
    "        else:\n",
    "            self.reward_vals = R\n",
    "\n",
    "    def get_moves(self, pos):\n",
    "        \"\"\"\n",
    "        Returns a list of the results of each action\n",
    "        N, E, S, W\n",
    "        \"\"\"\n",
    "        x = pos[0]\n",
    "        y = pos[1]\n",
    "        \n",
    "        move_list = []\n",
    "        \n",
    "        # north\n",
    "        if (x > 0) and (self.maze[x-1][y] != 1):\n",
    "            move_list.append((x-1,y))\n",
    "        else:\n",
    "            move_list.append((x, y))\n",
    "            \n",
    "        # east\n",
    "        if (y < self.y_bound) and (self.maze[x][y+1] != 1):\n",
    "            move_list.append((x, y+1))\n",
    "        else:\n",
    "            move_list.append((x, y))\n",
    "            \n",
    "        # south\n",
    "        if (x < self.x_bound) and (self.maze[x+1][y] != 1):\n",
    "            move_list.append((x+1, y))\n",
    "        else:\n",
    "            move_list.append((x, y))\n",
    "        \n",
    "        # west\n",
    "        if (y > 0) and (self.maze[x][y-1] != 1):\n",
    "            move_list.append((x, y-1))\n",
    "        else:\n",
    "            move_list.append((x, y))\n",
    "            \n",
    "        return move_list\n",
    "            \n",
    "    def print_maze(self):\n",
    "        print self.maze\n",
    "        \n",
    "    def get_transitions(self):\n",
    "        \n",
    "        n_states = self.maze.shape[0] * self.maze.shape[1]\n",
    "        # now I need to build a matrix that has dimensions (A, S, S)\n",
    "        # where\n",
    "        # - the first dimension is the action that you can take.\n",
    "        # - the second dimension is the starting state\n",
    "        # - the probabilities of ending up in other states\n",
    "        T = np.zeros((4, n_states, n_states))\n",
    "        \n",
    "        positions = [(x,y) for x in range(self.maze.shape[0]) for y in range(self.maze.shape[1])]\n",
    "\n",
    "        \n",
    "        for a in range(4):\n",
    "            for p_start in positions:\n",
    "                p_start_state = self.position_to_state(p_start)\n",
    "                \n",
    "                for p_end in positions:\n",
    "                    moves = self.get_moves(p_start)                    \n",
    "                    p_end_state =self.position_to_state(p_end)\n",
    "                    if (self.position_to_state(moves[a]) == p_end_state):\n",
    "                        T[a][p_start_state][p_end_state] = 1\n",
    "                        \n",
    "        T[:, self.position_to_state(self.goal), :] = 0\n",
    "        T[:, self.position_to_state(self.goal), self.position_to_state(self.start)] = 1\n",
    "            \n",
    "            \n",
    "  \n",
    "        return T\n",
    "            \n",
    "    def get_rewards(self):\n",
    "        n_states = self.maze.shape[0] * self.maze.shape[1]\n",
    "        # [reward for getting to goal, step penalty]\n",
    "        reward_vals = self.reward_vals\n",
    "        \n",
    "        # intialize with step penalties\n",
    "        R = np.ones((n_states, 4)) * reward_vals[1]\n",
    "        \n",
    "        # reward for goal state\n",
    "        R[self.position_to_state(self.goal),:] = reward_vals[0]\n",
    "        \n",
    "        # find the penalty spots in the maze\n",
    "        for x in range(self.maze.shape[0]):\n",
    "            for y in range(self.maze.shape[1]):\n",
    "                if self.maze[x][y] == 2:\n",
    "                    R[self.position_to_state((x,y)),:] = reward_vals[2]\n",
    "\n",
    "        return R\n",
    "    \n",
    "    def position_to_state(self, pos):\n",
    "        # (0,0) is state 0, (0, 1) is state 1, (0, 2) is state 2\n",
    "        # (1, 0) is state 3, (1, 1) is state 4\n",
    "        x = pos[0]\n",
    "        y = pos[1]\n",
    "        \n",
    "        return self.maze.shape[1] * x + y\n",
    "        \n",
    "    \n",
    "    def draw_maze(self, policy=None):\n",
    "        master = Tk()\n",
    "        DRAWING_HEIGHT = 800\n",
    "        DRAWING_WIDTH = 800\n",
    "        num_horizontal = maze.maze.shape[1]\n",
    "        num_vertical = maze.maze.shape[0]     \n",
    "        \n",
    "        BOX_WIDTH = DRAWING_WIDTH // num_horizontal\n",
    "        BOX_HEIGHT = DRAWING_HEIGHT // num_vertical\n",
    "        \n",
    "        w = Canvas(master, width=DRAWING_HEIGHT, height=DRAWING_WIDTH)\n",
    "        w.pack()\n",
    "\n",
    "        # draw the empty grid\n",
    "        for i in range(num_horizontal):\n",
    "            for j in range(num_vertical):\n",
    "                w.create_rectangle(i*BOX_WIDTH, j*BOX_HEIGHT, i*BOX_WIDTH+BOX_WIDTH, j*BOX_HEIGHT+BOX_HEIGHT, fill=\"white\")\n",
    "\n",
    "        # now want to loop through the maze and draw boxes the appropriate\n",
    "        # colour\n",
    "        x = 0\n",
    "        y = 0\n",
    "        for i in self.maze:\n",
    "            for j in i:\n",
    "                if j == 1:\n",
    "                    w.create_rectangle(y*BOX_WIDTH, x*BOX_HEIGHT, y*BOX_WIDTH+BOX_WIDTH, x*BOX_HEIGHT+BOX_HEIGHT, fill=\"black\")\n",
    "                if j == 2:\n",
    "                    w.create_rectangle(y*BOX_WIDTH, x*BOX_HEIGHT, y*BOX_WIDTH+BOX_WIDTH, x*BOX_HEIGHT+BOX_HEIGHT, fill=\"red\")\n",
    "                if j==3:\n",
    "                    w.create_rectangle(y*BOX_WIDTH, x*BOX_HEIGHT, y*BOX_WIDTH+BOX_WIDTH, x*BOX_HEIGHT+BOX_HEIGHT, fill=\"green\")\n",
    "                if (y, x) == self.start:\n",
    "                    w.create_rectangle(y*BOX_WIDTH, x*BOX_HEIGHT, y*BOX_WIDTH+BOX_WIDTH, x*BOX_HEIGHT+BOX_HEIGHT, fill=\"blue\")\n",
    "\n",
    "                y += 1\n",
    "            x += 1\n",
    "            y = 0\n",
    "            \n",
    "\n",
    "        if policy is not None:\n",
    "            padding = 5\n",
    "            x = 0\n",
    "            y = 0\n",
    "            for p in policy:\n",
    "                if p == 0:\n",
    "                    w.create_line(BOX_WIDTH/2 + (BOX_WIDTH*x), BOX_HEIGHT*y + padding, BOX_WIDTH/2 + (BOX_WIDTH*x), BOX_HEIGHT*y+BOX_HEIGHT - padding, arrow=FIRST, fill=\"orange\")\n",
    "                if p == 1:\n",
    "                    w.create_line(x * BOX_WIDTH + padding, BOX_HEIGHT/2 + (BOX_HEIGHT*y), x*BOX_WIDTH+BOX_WIDTH-padding, BOX_HEIGHT/2 + (BOX_HEIGHT*y), arrow=LAST, fill=\"orange\")\n",
    "                if p == 2:\n",
    "                    w.create_line(BOX_WIDTH/2 + (BOX_WIDTH*x), BOX_HEIGHT*y + padding, BOX_WIDTH/2 + (BOX_WIDTH*x), BOX_HEIGHT*y+BOX_HEIGHT - padding, arrow=LAST, fill=\"orange\")\n",
    "                if p == 3:\n",
    "                    w.create_line(x * BOX_WIDTH + padding, BOX_HEIGHT/2 + (BOX_HEIGHT*y), x*BOX_WIDTH+BOX_WIDTH-padding, BOX_HEIGHT/2 + (BOX_HEIGHT*y), arrow=FIRST, fill=\"orange\")\n",
    "\n",
    "                x += 1\n",
    "                if x == self.maze.shape[1]:\n",
    "                    x = 0\n",
    "                    y += 1\n",
    "                \n",
    "        mainloop()\n",
    "        \n",
    "    def qlearn(self, num_episodes=1000, gamma=.99, alpha_i=1, epsilon_i=1, epsilon_decay=0.999, alpha_decay=0.999):\n",
    "        \n",
    "        # initialize Q(s,a)\n",
    "        Q = np.random.rand(self.n_states, 4)\n",
    "        R = self.reward_vals\n",
    "\n",
    "\n",
    "        for i in range(num_episodes):\n",
    "            alpha = alpha_i\n",
    "            epsilon = epsilon_i\n",
    "            \n",
    "            for start_state in [(x,y) for x in range(self.maze.shape[0]) for y in range(self.maze.shape[1])]:\n",
    "                s = start_state\n",
    "                \n",
    "                while s != self.goal:\n",
    "                    self.q_iters += 1\n",
    "                    # make a copy of the array, we will compare this to the new Q \n",
    "                    # array after the iteration. If there is no significant change\n",
    "                    # we'll break \n",
    "                    old_q = np.empty_like(Q)\n",
    "                    old_q[:] = Q\n",
    "                    \n",
    "                    # There are some cases where there are no valid moves from the start\n",
    "                    # position. This trips things up. So break\n",
    "                    if maze.stuck_state(s):\n",
    "                        break\n",
    "            \n",
    "                    # choose an action from the current state\n",
    "                    current_state = self.position_to_state(s)\n",
    "                    action = self.epsilon_greedy(epsilon, Q[current_state])\n",
    "\n",
    "                    current_Q_val = Q[current_state][action]\n",
    "\n",
    "                    # now take the action\n",
    "                    new_state = self.simple_move(s, action)\n",
    "                    \n",
    "                    # get the reward for this new state\n",
    "                    if new_state == goal:\n",
    "                        r = R[0]\n",
    "                    elif self.maze[new_state[0]][new_state[1]]==2:\n",
    "                        r = R[2]\n",
    "                    else:\n",
    "                        r = R[1]\n",
    "\n",
    "                    new_state_num = self.position_to_state(new_state)\n",
    "                    \n",
    "                    # Reassign Q(s, a) with the correct value update function\n",
    "                    # get the Q value of the best action on the next state\n",
    "                    next_state_Q = np.max(Q[new_state_num])\n",
    "                    Q[current_state][action] = current_Q_val + (alpha * ( r+(gamma*next_state_Q) - current_Q_val))\n",
    "            \n",
    "                    # now consider the new state\n",
    "                    s = new_state\n",
    "                    \n",
    "                    # decay our epsilon and alpha, I'm not 100% sure this is the \n",
    "                    # proper place to do this. But it seems to converge on correct\n",
    "                    # ansert with this here\n",
    "                    epsilon *= epsilon_decay\n",
    "                    alpha *= alpha_decay \n",
    "\n",
    "                    # if there was no significant changes in Q, break.\n",
    "                    if np.allclose(old_q, Q, 0.0001, 0.0001):\n",
    "                        break \n",
    "        return Q\n",
    "        \n",
    "    def simple_move(self, pos, action):\n",
    "        \"\"\"\n",
    "        Deterministic. Given a position and an action return the one\n",
    "        position that will result\n",
    "        \"\"\"\n",
    "        x = pos[0]\n",
    "        y = pos[1]\n",
    "        \n",
    "        move_list = []\n",
    "        \n",
    "        # north\n",
    "        if action==0 and (x > 0) and (self.maze[x-1][y] != 1):\n",
    "            return (x-1,y)\n",
    "        \n",
    "        # east\n",
    "        if action == 1 and (y < self.y_bound) and (self.maze[x][y+1] != 1):\n",
    "            return (x, y+1)\n",
    "\n",
    "            \n",
    "        # south\n",
    "        if action == 2 and (x < self.x_bound) and (self.maze[x+1][y] != 1):\n",
    "            return (x+1, y)\n",
    "        \n",
    "        # west\n",
    "        if action == 3 and (y > 0) and (self.maze[x][y-1] != 1):\n",
    "            return (x, y-1)\n",
    "        \n",
    "        return (x, y)\n",
    "\n",
    "    def stuck_state(self, pos):\n",
    "        \"\"\"\n",
    "        we randomly choose a state to explore. This could be\n",
    "        in a state that only has walls. We want to do a random restart \n",
    "        if we're in one of those states\n",
    "        \"\"\"\n",
    "\n",
    "        n = self.simple_move(pos, 0)\n",
    "        e = self.simple_move(pos, 1)\n",
    "        s = self.simple_move(pos, 2)\n",
    "        w = self.simple_move(pos, 3)\n",
    "\n",
    "        return n == e == s == w == pos\n",
    "    \n",
    "    def epsilon_greedy(self, epsilon, actions):\n",
    "        \"\"\"\n",
    "        Return the max, or not, w/e\n",
    "        \"\"\"\n",
    "        rand = np.random.random()\n",
    "\n",
    "        if rand <= epsilon:\n",
    "            # return a random action\n",
    "            return np.random.randint(0, 4)\n",
    "        else:\n",
    "            # return the max action\n",
    "            return np.argmax(actions)\n",
    "        \n",
    "    def find_reward(self, policy):\n",
    "        \"\"\"\n",
    "        Given the policy, find the total reward that we get by following that \n",
    "        policy from the start state\n",
    "        \"\"\"\n",
    "        current_position = self.start\n",
    "        current_reward = 0\n",
    "        \n",
    "        while current_position != self.goal:\n",
    "            state_num = self.position_to_state(current_position)\n",
    "            \n",
    "            # look up in the policy what to do at this position\n",
    "            action = policy[state_num]\n",
    "            \n",
    "            current_position = self.simple_move(current_position, action)\n",
    "    \n",
    "            if self.maze[current_position[0]][current_position[1]] == 0:\n",
    "                current_reward += self.reward_vals[1]\n",
    "            elif self.maze[current_position[0]][current_position[1]] == 2:\n",
    "                current_reward += self.reward_vals[2]\n",
    "            elif self.maze[current_position[0]][current_position[1]] == 3:\n",
    "                current_reward += self.reward_vals[0]\n",
    "        \n",
    "\n",
    "        return current_reward\n",
    "\n",
    "            \n",
    "            \n",
    "            \n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from mdptoolbox.mdp import ValueIteration\n",
    "from mdptoolbox.mdp import PolicyIteration\n",
    "from pybrain.rl.environments.mazes import Maze, MDPMazeTask\n",
    "from pybrain.rl.learners import Q\n",
    "from pybrain.rl.learners.valuebased import ActionValueTable\n",
    "\n",
    "from Tkinter import *\n",
    "\n",
    "themaze = np.asarray([\n",
    "        [0,1,0,0,0],\n",
    "        [0,1,0,2,0],\n",
    "        [0,2,0,1,0],\n",
    "        [0,1,0,1,0],\n",
    "        [0,0,0,1,3]\n",
    "        \n",
    "    ])\n",
    "goal = (4,4)\n",
    "start = (0,0)\n",
    "\n",
    "rewards1 = [1000, -0.05, -3]\n",
    "rewards2 = [1000, -5, -20]\n",
    "rewards3 = [0, 1, -2]\n",
    "maze = RLMaze(R=rewards1)\n",
    "# maze = RLMaze(maze=themaze, goal=goal, start=start, R=rewards3)\n",
    "# maze.draw_maze()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VITER REWARD 995.15\n",
      "VITER TIME 1.45050501823\n",
      "VITER ITERS 131\n",
      "PITER REWARD 995.15\n",
      "PITER TIME 44.8407440186\n",
      "PITER ITERS 81\n",
      "Q LEARN"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-35-503dd41fe8b5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0mq_policy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m \u001b[0;32mprint\u001b[0m \u001b[0;34m\"Q LEARN\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmaze\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind_reward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq_policy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m \u001b[0;32mprint\u001b[0m \u001b[0;34m\"Q LEARN TIME\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;32mprint\u001b[0m \u001b[0;34m\"Q ITERS\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmaze\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mq_iters\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-33-0be36c9b2b86>\u001b[0m in \u001b[0;36mfind_reward\u001b[0;34m(self, policy)\u001b[0m\n\u001b[1;32m    379\u001b[0m             \u001b[0mcurrent_position\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msimple_move\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcurrent_position\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    380\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 381\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmaze\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcurrent_position\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcurrent_position\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    382\u001b[0m                 \u001b[0mcurrent_reward\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreward_vals\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    383\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmaze\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcurrent_position\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcurrent_position\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "T = maze.get_transitions()\n",
    "R = maze.get_rewards()\n",
    "discount = 0.90\n",
    "\n",
    "value_iteration = ValueIteration(T, R, discount)\n",
    "value_iteration.run()\n",
    "print \"VITER REWARD\", maze.find_reward(value_iteration.policy)\n",
    "print \"VITER TIME\", value_iteration.time\n",
    "print \"VITER ITERS\", value_iteration.iter\n",
    "maze.draw_maze(value_iteration.policy)\n",
    "\n",
    "policy_iteration = PolicyIteration(T,R, discount)\n",
    "policy_iteration.run()\n",
    "print \"PITER REWARD\", maze.find_reward(policy_iteration.policy)\n",
    "print \"PITER TIME\", policy_iteration.time\n",
    "print \"PITER ITERS\", policy_iteration.iter\n",
    "maze.draw_maze(policy_iteration.policy)\n",
    "\n",
    "\n",
    "s = time.time()\n",
    "Q = maze.qlearn()\n",
    "n = time.time()\n",
    "q_policy = []\n",
    "for state in Q:\n",
    "    q_policy.append(np.argmax(state))\n",
    "    \n",
    "print \"Q LEARN\", maze.find_reward(q_policy)\n",
    "print \"Q LEARN TIME\", (n-s)\n",
    "print \"Q ITERS\", maze.q_iters\n",
    "maze.draw_maze(q_policy)\n",
    "\n",
    "# get policy from Q\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ -0.05,  -0.05,  -0.05,  -0.05],\n",
       "       [ -0.05,  -0.05,  -0.05,  -0.05],\n",
       "       [ -0.05,  -0.05,  -0.05,  -0.05],\n",
       "       [ -0.05,  -0.05,  -0.05,  -0.05],\n",
       "       [ -0.05,  -0.05,  -0.05,  -0.05],\n",
       "       [ -0.05,  -0.05,  -0.05,  -0.05],\n",
       "       [ -0.05,  -0.05,  -0.05,  -0.05],\n",
       "       [ -0.05,  -0.05,  -0.05,  -0.05],\n",
       "       [ -1.  ,  -1.  ,  -1.  ,  -1.  ],\n",
       "       [ -0.05,  -0.05,  -0.05,  -0.05],\n",
       "       [ -0.05,  -0.05,  -0.05,  -0.05],\n",
       "       [ -1.  ,  -1.  ,  -1.  ,  -1.  ],\n",
       "       [ -0.05,  -0.05,  -0.05,  -0.05],\n",
       "       [ -0.05,  -0.05,  -0.05,  -0.05],\n",
       "       [ -0.05,  -0.05,  -0.05,  -0.05],\n",
       "       [ -0.05,  -0.05,  -0.05,  -0.05],\n",
       "       [ -0.05,  -0.05,  -0.05,  -0.05],\n",
       "       [ -0.05,  -0.05,  -0.05,  -0.05],\n",
       "       [ -0.05,  -0.05,  -0.05,  -0.05],\n",
       "       [ -0.05,  -0.05,  -0.05,  -0.05],\n",
       "       [ -0.05,  -0.05,  -0.05,  -0.05],\n",
       "       [ -0.05,  -0.05,  -0.05,  -0.05],\n",
       "       [ -0.05,  -0.05,  -0.05,  -0.05],\n",
       "       [ -0.05,  -0.05,  -0.05,  -0.05],\n",
       "       [ 10.  ,  10.  ,  10.  ,  10.  ]])"
      ]
     },
     "execution_count": 290,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
